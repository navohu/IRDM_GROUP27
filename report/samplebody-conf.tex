\section{Introduction}

TODO:

\section{Related Work}
Creating an ranking results in a search engine can be done through many different algorithms. This literature review will go through two type of algorithms used for ranking the documents based on the query: non-probabilistic and probabilistic. Then we will explore how to evaluate the results, based on a test set. Deep Learning is also a new topic that has caught attention, and will therefore be reviewed in terms of how it can be used in search engines.

\subsection{TF-IDF} % (fold)
\label{sub:tf_idf}

Term Frequency (TF) and Inverse Document Frequency (IDF)  are used to examine the good and bad discriminators. 

TF is the number of occurrences of a term in the document. For a document, the set of weights determined by the TF is considered as a quantitative digest of that document. In this case, the ordering of each term in the document is ignored but the number of occurrences is noted down.

IDF is defined to be the total number of occurrences of a term in a collection of documents. Since TF considers all terms have equal importance when it comes to assessing the relevance of a query, this might be a problem since some terms have little or no discriminating power in determining relevance.

To produce a composite weight for each term in each document, the definitions of term frequency and inverse document frequency are combined and the weighting scheme is as:

\[tf\cdot idf_{t,d}=tf_{t,d}\times idf_{t}\]

where t stands for term and d stands for document. Therefore, the total relevance score of a document would be:

\[score(q,d)=\sum_{t\in q}^{} tf\cdot idf_{t,d}\]

% subsection tf_idf (end)

\subsection{BM25} % (fold)
\label{sub:bm25}

BM25 improves upon TF*IDF. It stands for Best Match 25. BM Okapi 25 is designed for modern full-text search collection since it pays additional attention to the term frequency and document length.

Classic IDF has the potential for giving negative scores for terms with very high document frequency. So IDF in BM25 adds 1 to the value, before taking the log, which makes it impossible to compute a negative value.

Term frequency in BM25 dampens the impact of term frequency even further than traditional TF*IDF. The impact of term frequency is always increasing, but asymptotically approaches a value. In BM25, the TF score also depends on the document length. If the document length is below the average, then the TF score will get a bonus. 

The formula to adjust the TF score based on the document length is as below where b and k are constants and L is the length of document:
\[\frac{(k + 1) * TF}{k * (1.0 - b + b * L) + TF}\]

Adding up all the changes upon the classic TF-IDF, the formula of BM25 is as follow:
\[score(q,d)=IDF * \frac{(k + 1) * TF}{k * (1.0 - b + b * \frac{|d|}{avgDl}) + TF}
\]

% subsection bm25 (end)

\subsection{Probabilistic Algorithms} % (fold)
\label{sub:probabilistic_algorithms}

% subsection probabilistic_algorithms (end)

\subsection{Evaluation} % (fold)
\label{sub:evaluation}

% subsection evaluation (end)

\subsection{Deep Learning} % (fold)
\label{sub:deep_learning}

% subsection deep_learning (end)

\section{Experiment} % (fold)
\label{sec:experiment}

\subsection{Dataset Description} % (fold)
\label{sub:dataset_description}

% subsection dataset_description (end)

\subsection{Methods} % (fold)
\label{sub:methods}

% subsection methods (end)

\subsection{Metrics \& Analysis} % (fold)
\label{sub:metrics_&_analysis}

% subsection metrics_&_analysis (end)

\subsection{Analysis of Results} % (fold)
\label{sub:analysis_of_results}

% subsection analysis_of_results (end)

% section experiment (end)

\section{Discussion \& Limitations} % (fold)
\label{sec:discussion_&_limitations}

% section discussion_&_limitations (end)

\section{Conclusion} % (fold)
\label{sec:conclusion}

% section conclusion (end)

\section{References} % (fold)
\label{sec:references}

% section references (end)


% \begin{equation}
%   \lim_{n\rightarrow \infty}x=0
% \end{equation}
% Notice how it is formatted somewhat differently in
% the \textbf{displaymath}
% environment.  Now, we'll enter an unnumbered equation:
% \begin{displaymath}
%   \sum_{i=0}^{\infty} x + 1
% \end{displaymath}
% and follow it with another numbered equation:
% \begin{equation}
%   \sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f
% \end{equation}
% just to demonstrate \LaTeX's able handling of numbering.


% \begin{table}
%   \caption{Frequency of Special Characters}
%   \label{tab:freq}
%   \begin{tabular}{ccl}
%     \toprule
%     Non-English or Math&Frequency&Comments\\
%     \midrule
%     \O & 1 in 1,000& For Swedish names\\
%     $\pi$ & 1 in 5& Common in math\\
%     \$ & 4 in 5 & Used in business\\
%     $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
%   \bottomrule
% \end{tabular}
% \end{table}


% \begin{table*}
%   \caption{Some Typical Commands}
%   \label{tab:commands}
%   \begin{tabular}{ccl}
%     \toprule
%     Command &A Number & Comments\\
%     \midrule
%     \texttt{{\char'134}author} & 100& Author \\
%     \texttt{{\char'134}table}& 300 & For tables\\
%     \texttt{{\char'134}table*}& 400& For wider tables\\
%     \bottomrule
%   \end{tabular}
% \end{table*}
% % end the environment with {table*}, NOTE not {table}!

% Lists:
% \begin{enumerate}
% \item Never, ever use vertical rules.
% \item Never use double rules.
% \end{enumerate}
% It is also a good idea not to overuse horizontal rules.

% Figures
%\begin{figure}
%\includegraphics{fly}
%\caption{A sample black and white graphic.}
%\end{figure}
%
%\begin{figure}
%\includegraphics[height=1in, width=1in]{fly}
%\caption{A sample black and white graphic
%that has been resized with the \texttt{includegraphics} command.}
%\end{figure}



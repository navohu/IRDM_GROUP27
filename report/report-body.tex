\section{Introduction}

The end of the 20th century gave rise to a need to quickly obtain information on the World Wide Web. As a result of this, search engines were developed and are one of the most common methods to find information online today. Competition between different search engines has been present since the start, but Google has long ruled the market. Its has become widely used because its algorithms have outperformed those of other search engines. Not all of the algorithms that Google is using are publicly available, but those that are known have been present through decades, such as TF-IDF, BM25, PageRank and query likelihood ranking.

The aim of this report is to explore existing algorithms for information retrieval and how they perform against each other. The search engine was implemented on the "cs.ucl.ac.uk" domain and includes several preprocessing techniques and data transformations. The application includes implementations of several ranking algorithms and an evaluation of the results. The search engine application built for this project can be found on \url{github.com/navohu/IRDM_GROUP27} and has been implemented in Python\footnote{All instructions for running the application can be found in the README file in the repository.}. Initially, our expectations for the project were to produce results that showed the difference of the current working algorithms, as well as an implementation of a deep learning algorithm. Since our project only included a search on the CS website it would imply that the amount of data might not be sufficient for a deep learning approach.



TODO: Our results ... compare the different ranking algorithms with and without PageRank. Explain what metric showed the best results.


This report will first explain different discoveries within the field of information retrieval, followed by an experiment section. This section will explore the methodology of how we created the search engine like setting up an inverted index, programming the crawler and implementing the algorithms. After the methodology section we will introduce the evaluation methods and metrics we used. Finally, we will analyse our results and provide a discussion about the project and its limitations.


\section{Related Work}
Retrieving and ranking results in a search engine can be done through many different algorithms. In this section, we will explore the literature covering two categories of ranking algorithms: probabilistic and non-probabilistic. We will then investigate how the performance of an algorithm is commonly evaluated given a test set. Furthermore, we will review Deep Learning, a new topic that has caught attention within information retrieval, and how it can be used in search engines.

Today, web search is a tool that individuals, universities and businesses use on a daily basis. Many of these retrieval systems fall under traditional information retrieval (IR). The first web search engine was created in 1993, but soon the renowned search engine Google was created by Larry Page and Sergey Brin. The feature that made Google better than its competitors was an algorithm called PageRank introduced by Page in 1998 \cite{brin1998anatomy}. He describes PageRank as "an objective measure of [a website's] citation importance that corresponds well with people's subjective idea of importance." This ranking algorithm is still used by Google today.

Another method of ranking the importance of a website was developed by Kleinberg in 1999 \cite{kleinberg1999authoritative}. He proposed a link analysis algorithm called HITS. It involves identifying hubs and authorities in a web graph, where hubs serve as large directories without specific authoritative information, while authorities hold important information. % TODO: clarify?

Both PageRank and HITS are algorithms that rank the importance of a page, but do not take a query into consideration.
% TODO: clarify the sentence below
Hence, these algorithms are good for ranking the difference between results returned by algorithms that take the query into consideration.
Within query-dependent rankers, one of the earliest and simplest non-probabilistic algorithms was the Standard Boolean model introduced by Fayen \cite{lancaster1973fayen}. Since this algorithm is based on an exact match between the query terms and the document content, it will often either return too many or too few documents. This simple model was improved upon with more sophisticated algorithms such as the Vector Space Model \cite{salton1975vector}, TF-IDF \cite{salton1983mcgill}, and query likelihood language models \cite{zhai2001model} among others. They all have a goal of retrieving the most relevant documents to a certain query.

IR research has always had a strong emphasis on measuring the efficacy of an IR system. This includes discovering the relevance of documents, retrieved by a search engine, relative to the user's information need. Evaluation is important in order to assess the performance of the system, measure the differences of multiple systems, and to learn the faults of a system with regards to improve the works.  The Text Retrieval Conference (TREC) is a conference emphasising on supporting and persuading the IR community through providing an infrastructure for evaluation IR systems. The famous infrastructure for evaluation, trec\_eval, is used for all ad hoc tasks in TREC \cite{voorhees:evaluation}. Two of the measurements are precision and recall, first developed by Spärck-Jones, K.\cite{jones1981information}. This type of evaluation is based on a document collection that includes documents which are classified as relevant or non-relevant, based on a set of queries.

On the other hand, it has shown that defining relevance is very hard when it comes to "real" users\cite{mizzaro1997relevance}. Therefore, another method of capturing user interactions with a search engine is Transaction Log Analysis (TLA). It is based on creating transaction logs to recognise attributes within the search process. It's a way of measuring the searcher's actions, the interaction between the user and the system along with the results \cite{glaser1967discovery}. 

Having a stable and robust test collection to help evaluate your IR model has been an important part of evaluation. It has created a more connected research society where researchers have shared test collections between each other. Mark Sanderson emphasises the powerfulness of good test collections used in conjunction with evaluation measures \cite{sanderson:evaluation}. One evaluation method mentioned by Sanderson is Discounted Cumulative Gain (DCG) and normalised DCG (nDCG)\cite{sanderson:evaluation}. DCG is often used to measure the ranking quality and the effectiveness of web-based search engines specifically. The algorithm measures the ‘gain’ of a document based on its position in the list. nDCG normalises the gain across queries, which is done by sorting the relevant documents by their relative relevance.

The very popular subject Deep Learning has also been used in web search engines. Huang, PS. \emph{et al.} proposes a deep structured semantic model that is trained by trying to reach the maximum conditional likelihood of a clicked document given a query \cite{huang2013learning}. The idea is based on click-through data collected which is used to feed the model in order to learn which pages are relevant to a query. Another paper by Deng, L. \emph{et al.} used deep stacking networks (DNS) in order to perform a parallel and scalable relevance prediction on an IR task \cite{deng2013deep}. It outperforms previous well known machine learning ranking algorithms such as LambdaRank and LambdaMART \cite{burges2010ranknet} in normalised DCG. 

Many of these algorithm are commonly used in web search today and the rest of the report will explore how some of these techniques can be used in a search engine. A comparison between different methods will also be provided.


% section related_work (end)


\section{Experiment} % (fold)
\label{sec:experiment}

This section will present the overall methodology of building a search engine. The whole methodology is freely and easily adaptable since it's using open source Python libraries and a free instance of an Amazon server. There are five main algorithms that have been implemented and compared against in this project: Boolean Retrieval, TFIDF, BM25, query likelihood estimation and PageRank. The assortment of algorithms were chosen by a thorough examination of the current literature in the field. We wanted to test algorithms that were known to be both very accurate and some that were less accurate in order to receive a full view of the the most known algorithms in the field, and the main differences between them. 

\subsection{Dataset Description} % (fold)
\label{sub:dataset_description}

The search engine is dealing with two core datasets: the crawled web pages and the test set for evaluation. The crawled web pages are specifically modified and transformed for use. One example of this is creating an inverted index based on the content of the crawled web pages. Another example is how to create an adjacency matrix based on the relationship between pages.

The test set was manually created in order for us to evaluate the performance of our search engine. Two main test sets were created: one for precision, and one for recall. That means that one test set is specifically created such that the user want \emph{one} specific document (precision), while the other test set is created to retrieve as many relevant documents as possible (recall). Both test sets needed queries and relevant documents related to the queries. There are also three different types of queries: informational, transactional and navigational queries. The two test sets therefore includes all three types of queries.

% subsection dataset_description (end)

\subsection{Methods} % (fold)
\label{sub:methods}
The methodology includes a wide range of steps and exploration in order to build our search engine. These will be presented here.

\subsubsection{AWS Database \& Server Setup} % (fold)
\label{ssub:database_and_server_setup}

The search engine is run on an Amazon EC2 instance that provided secure and resizeable computing capacity. It is usually good practice to run heavy loaded applications - such as a search engine - on a cloud computing service. The computational power is stronger than on running it locally. Another advantage is that it's possible to set up a database instance which can be accessed by anyone. Our project is using a PostgreSQL database within the EC2 server.

Our database has been created in such a way that it is easy for our application to access the data when processing the rankings. Our crawler would write to a table called \emph{cs\_cites} and is built to hold these entities:

\begin{center}
  \begin{tabular}{|c|c|c|}
  id & title & link
  \end{tabular}
\end{center}

Later on in the progress, an inverted index was created and would eventually add additional columns to the \emph{cs\_sites} table. The inverted index consisted of two tables in the database. One table would consist of the relationship between $word\_id$, the $document\_id$ and the word occurrence of that word within that document, like shown in the table below. The \emph{document\_id} is taken from the \emph{cs\_sites} table.

\begin{table}[h!]
  \begin{tabular}{|c|c|c|}
  \hline
  word\_id & document\_id & occurrences \\ \hline 
  1 & 4 & 2 \\\hline
  1 & 2 & 5 \\\hline
  2 & 1 & 1 \\\hline
  3 & 1 & 4 \\\hline
  3 & 3 & 6 \\\hline
  3 & 2 & 3 \\\hline
  ... & ... & ...
  \end{tabular}
\end{table}

The \emph{word\_id} is mapped within yet another table called \emph{cs\_dictionary} and has the structure as shown in the table below. The \emph{frequency} is the number of word occurrences in the whole collection.

\begin{table}[h!]
  \begin{tabular}{|c|c|c|}
  \hline
  word\_id & word & frequency \\ \hline 
  1 & chemistry & 7 \\\hline
  2 & computer & 1 \\\hline
  3 &science & 13 \\\hline
  ... & ... & ...
  \end{tabular}
\end{table}

% subsubsection database_and_server_setup (end)

\subsubsection{Crawling} % (fold)
\label{ssub:crawling}

The crawling was done through an open source platform called Scrapy\footnote{https://scrapy.org} written in Python. This platform would be given a start-url "\url{www.cs.ucl.ac.uk}", followed by a depth-first-search on all links containing the "\url{cs.ucl.ac.uk}" domain. So "\url{www.cs.ucl.ac.uk/home}" would be accepted, just like "\url{www.wiki.cs.ucl.ac.uk}". We set a max-depth of the crawler because we realised throughout the crawling that some links were extremely long with unnecessary indexing information of the CS website.

The elements that were crawled from the website was the urls, titles ($<$h1$>$ tags) of page and the anchor text. Each instance would also be associated with a unique identifier. Let's call this table \emph{cs\_sites}. The reason why we didn't crawl the content of the pages to store in the database was because of storage reasons. As long as we have all the links of the whole domain in the database it is easy to access the content of the page at a later point.

% subsubsection crawling (end)

\subsubsection{Inverted Index} % (fold)
\label{ssub:inverted_index}

Explain the columns that were added to the cs\_sites table

% subsubsection inverted_index (end)

\subsubsection{PageRank} % (fold)
\label{ssub:pagerank}

The popular algorithm PageRank\footnote{https://en.wikipedia.org/wiki/PageRank} is an algorithm developed by Google for ranking the results within a search engine. It is also commonly used for measuring the importance of a website. The algorithm is based on an assumption that more important websites are more likely to be pointed to from other websites.

The algorithm implies that it would need a way of representing connections between web sites. This can easily be done with the aid of an adjacency matrix. Firstly, all the connections needed to be measured, which was done by obtaining all the outgoing links for each site in \emph{cs\_sites}. Now, each link would point to a certain number of outgoing links disregarding whether they were in the \emph{cs.ucl.ac.uk} domain or not. From these connection an adjacency matrix was produces as shown as an example in Table \ref{fig:adj_mx}. 

\begin{table}[!h]
  \centering
  \begin{tabular}{|lr|c|c|c|} \cline{3-5}
  \multicolumn{1}{l}{} && \multicolumn{3}{c|}{To} \\ \cline{3-5}
  \multicolumn{1}{l}{} & & A & B & C  \\ \hline
  \multirow{3}{*}{\begin{sideways}From\end{sideways}}
  %                           A   B   C   
  & \multicolumn{1}{|r|}{A} & 0 & 0 & 1  \\ \cline{2-5}
  & \multicolumn{1}{|r|}{B} & 0 & 1 & 1  \\ \cline{2-5}
  & \multicolumn{1}{|r|}{C} & 1 & 1 & 0  \\ \hline
  \end{tabular}
  \caption{Adjacency matrix between links, where A, B and C represents links. The number 1 represents a connection, while 0 represents no connection.}
  \label{fig:adj_mx}
\end{table}

The adjacency matrix was then used in the PageRank algorithm to compute the rank of each link. The implementation would first convert the adjacency matrix into a markov matrix, followed by computing the rank of state \emph{i}:

$$ r_i = r_{i-1} \cdot (I_i *s + S_i * s + T_i * (1-s)) $$ 

where \emph{r} is the PageRank, \emph{$I_i$} is the inlinks of state \emph{i}, \emph{$S_i$} are the sink states, \emph{$T_i$} is the teleporting state and \emph{s} is the probabillity of following a transitions. That means that \emph{s-1} is the probability of teleporting. We have set $s=.85$. The algorithm will continue to calculate \emph{$r_i$} until it reaches a value below the maxerr rate ($maxerr = 0.001$). It is then said to be converged. 

The PageRank is then used in our search engine to rank the difference between similar documents. If two documents showed to have very similar ranks produced, the PageRank can be a useful metric that can give an extra weight to each ranked document. It will then be clearer which documents are more appropriate and most likely more visited. 


% subsubsection pagerank (end)

\subsubsection{Boolean Retrieval} % (fold)
\label{ssub:boolean_retrieval}

% subsubsection boolean_retrieval (end)

\subsubsection{TF-IDF} % (fold)
\label{sub:tf_idf}

TF-IDF is a type of ranking algorithm that is based off of reflecting how important a certain word is within a document in a collection. Normally, it is used as a weighting factor but can also be used for pure ranking given a query term. It consists of two main concepts:  term frequency (TF) and inverse document frequency (IDF). The two terms have been implemented in the search engine using these equations:

$$TF = \frac{f_{t,d}}{\sum\limits_{t' \in d} f_{t,d}}$$

$$IDF = log_2 \frac{N}{n_t}$$

where $f_{t,d}$ is the number of term occurrences within a document, while the summation is the length of the document. \emph{N} is the number of documents in collection and $n_t$ is the term frequency in the collection. The documents would then be ranked based on the product of $TF * IDF$ and will display all the documents in order. 

% subsection tf_idf (end)

\subsubsection{BM25} % (fold)
\label{ssub:BM25}

BM25 is another type of ranking algorithm developed by Karen Spärck Jones. It is a newer and improved version of TF-IDF. The BM25 belongs to the probabilistic models and uses the "bag of words" concept for retrieving relevant documents from a given search query. The formula to adjust the TF score based on the document length is as below where b and k are constants and L is the length of document:

$$ score(D, Q) = \sum_{i=1}^n IDF(q_i) * \frac{f(q_i, D)* (k + 1)}{f(q_i, D) + k * (1-b + b * \frac{|D|}{avgdl})}$$

where the IDF is defined as in subsection \ref{sub:tf_idf}.

% subsubsection BM25 (end)

\subsubsection{Query Likelihood with Smoothing} % (fold)
\label{ssub:query_likelihood_with_smoothing}

% subsubsection query_likelihood_with_smoothing (end)

All of these algorithms could be run within our app by choosing a parameter when executing the Python script. This way it is quick to see the differences between the different algorithms and to see whether they would give different results. A more systematic approach to measure the performance of the different ranking algorithms is through proper evaluation. This is explained in the next section.


% subsection methods (end)

\subsection{Metrics \& Analysis} % (fold)
\label{sub:metrics_&_analysis}

We conducted a series of evaluation tests on TF-IDF, Query Likelihood Ranking and BM25 to evaluate the performances of our search engine.
\subsubsection{Queries}
Commonly, there are three types of search query people type into the search box which are 'informational queries', 'navigational queries' and 'transaction queries'. 

Informational queries are normally defined as 'Queries that cover a broad topic (e.g. colorado or trucks) for which there may be thousands of relevant results.' \cite{queries}. Those queries are normally used when  the user is looking for the information relate to the query rather than a specific website or transactional function. Therefore, more relevant documents are retrieved by the informational queries than the transactional and navigational queries. For the following tests, we would use 'jun wang' and 'degree' as the informational queries. The reason giving to queries is that 'jun wang' is more specific than 'degree', therefore we could compare the performances of engines with specific and general queries.

Navigational queries are frequently used when someone is trying to find a certain website or portal. For example, a user might enter 'Facebook' into the search bar to find the Facebook website rather than directly entering the url into the browser's navigational bar. Therefore, the navigational results should be ranked on the top for this kinds of queries. However, some queries that seem to be navigational in nature but might be not. For example, users who search for 'facebook' might actually be looking for news of facebook. We decided to use 'moodle' to represent this kind of queries.

A Transactional query shows an intend to complete a transaction, e.g. Making a purchase or paying fees. We skipped this query since there are too few transactional links under cs.ucl.ac.uk domain. 

\subsubsection{Recall, Precision and F1}
Recall, precision and F1 score are the most popular metrics to evaluate the performance of a search engine. 

Recall is the ratio of the number of relevant documents retrieved by the search engine to the number of total relevant document in the database. The formula is shown as below: 
\[recall=\frac{\mid relevant documents\cap retrieved documents\mid}{\mid relevant documents \mid}\]

However, it does not make sense to pursue 100\% recall by returning all documents in response to every query, therefore, recall alone is not enough but one needs to measure the non-relevant document retrieved. Precision is a good supplementary metric. It measures the ratio of the number of relevant  documents retrieved to the total number of document retrieved by the search engine. The formula for precision is shown as below:
\[precision=\frac{\mid relevant documents\cap retrieved documents\mid}{\mid retrieved documents \mid}\]

F1 score is a good measure of accuracy of the result of the given search engine. It considers both precision and recall of the test to compute the score. The general formula is F1 score is as follow:
\[F1=(1+\beta^{2})\cdot \frac{precision\cdot recall}{(\beta^{2}\cdot precision)+recall}\]

In our experiment, we would set the parameter to be the traditional value 0.5, and therefore the formula would be:

\[F1=2\cdot \frac{precision \cdot recall}{precision+recall}\]
The following tables indicate the results of precision, recall and F1 score of tests on queries 'degree', 'jun wang' and 'moodle' respectively. There are totally 68, 10 and 2 relevant documents in the queries of 'degree', 'jun wang' and 'moodle' test pools. 

It can be seen that the BM25 has the best performance on both precision and recall among all three algorithms. This could because that the penalty on term frequency and document length that BM25 implements helps to avoid retrieving non-relevant documents. Also, it strikes to the eyes that specific query 'jun wang' gives higher recall while lower precision performance than the general query 'degree'. This could because the specificity of the query restricts the total number of relevant documents and therefore search engines would be easier to retrieve most of those documents. However, a general query allows more documents to be relevant, and hence the precision would be higher. 

For the query 'moodle', since its nature of being navigational, there should be only a few relevant results corresponding to it. Therefore, the recall of the performances of all engines is high although the precision is low since engines will start to give less relevant results once all highly relevant results have been retrieved.

\begin{table}[]
\centering
\caption{query 'degree'}
\label{my-label}
\begin{tabular}{|l|l|l|l|}
\hline
          & BM25  & QueryLikelihood & TF-IDF \\ \hline
Precision & 0.80  & 0.733           & 0.733  \\ \hline
Recall    & 0.369 & 0.338           & 0.338  \\ \hline
F1 score  & 0.505 & 0.463           & 0.463  \\ \hline
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{query 'jun wang'}
\label{my-label}
\begin{tabular}{|l|l|l|l|}
\hline
          & BM25  & QueryLikelihood & TF-IDF \\ \hline
Precision & 0.333 & 0.233           & 0.267  \\ \hline
Recall    & 1     & 0.70            & 0.8    \\ \hline
F1 score  & 0.5   & 0.35            & 0.40   \\ \hline
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{query 'moodle'}
\label{my-label}
\begin{tabular}{|l|l|l|l|}
\hline
          & BM25 & QueryLikelihood & TF-IDF \\ \hline
Precision & 0.4  & 0.4             & 0.4    \\ \hline
Recall    & 1    & 1               & 1      \\ \hline
F1 score  & 0.2  & 0.2             & 0.2    \\ \hline
\end{tabular}
\end{table}

Graph \textbf{XYZ } show the plots of precision and recall of the performances of three queries respectively. It can be seen that BM25 could always retrieve more relevant documents than the other two algorithms which proves that the penalty  on term frequency and document length is very useful while determine the relevance of a document. Furthermore, the performances of Query Likelihood and TF-IDF are very close. Further tests on results ranking and DCG might give more detailed differences between performances between these two methods.

Overall, the performance of the BM25 is the best considering all of the metrics of precision, recall and F1. 

\subsubsection{Discounted Cumulative Gain( DCG)}

DCG is a measure of ranking quality. It is used to evaluate the ranking of retrieved documents based on their position in the result list. The gain is accumulated from the top of the list to the bottom with the gain of each result discounted at lower ranks. The higher the DCG value is, the better the ranking is, and therefore more relevant documents are ranked on the top. The formula is as follow:

\[DCG_{k}=\sum_1^k \frac{2^{rel_{i}}-1}{log_{2}(i+1)}\]
We have carried out tests of three search terms with our three algorithms. Table \textbf{X} shows that BM25 has significantly better performance on search term $degree$ and $junwang$, while all three algorithms perform at the same level with search term $moodle$. We could interpret that BM25 is the best algorithm among those three, however, for search $moodle$, the amount of retrieved relevant URLs is too small to differentiate the performance of the algorithms. QueryLikehood outperforms TF-IDF on search term $degree$, by contrast, TF-IDF is the winner with search term $junwang$. Overall, those two algorithms deliver approximately the same level relevancy.


\begin{table}[]
\centering
\caption{DCG of three queries}
\label{my-label}
\begin{tabular}{|l|l|l|l|}
\hline
        & BM25  & QueryLikelihood & TF-IDF \\ \hline
degree  & 12.16 & 10.3            & 9.37   \\ \hline
junwang & 7.25  & 6.3             & 6.6    \\ \hline
moodle  & 4     & 4               & 4      \\ \hline
\end{tabular}
\end{table}

This project's aim was to rank and compare already existing ranking algorithms. The evaluation is an important step to measure performance and will show expected results if the algorithms were implemented correctly. The most common metric in evaluation within information retrieval is precision and recall, which is one of the main evaluation algorithms used within this project. They are both inversely related which means that a high precision will normally give a lower recall and vice versa. They are defined as follows:

$$precision = \frac{|relevant documents \cap retrieved documents|}{|retrieved documents|}$$

$$recall = \frac{|relevant documents \cap retrieved documents|}{|relevant documents|}$$

% subsection metrics_&_analysis (end)

\subsection{Analysis of Results} % (fold)
\label{sub:analysis_of_results}

% subsection analysis_of_results (end)

% section experiment (end)

\section{Discussion \& Limitations} % (fold)
\label{sec:discussion_&_limitations}

Different type of implementation of TF-IDF and BM25, but we have only chosen some. 
Limitations about the crawler - did it really get everything? Depth issue.

Hard to show good differences when the test result was this small.

% section discussion_&_limitations (end)

\section{Conclusion} % (fold)
\label{sec:conclusion}

% section conclusion (end)

\section{References} % (fold)
\label{sec:references}

% section references (end)



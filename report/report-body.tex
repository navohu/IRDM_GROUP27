\section{Introduction}

TODO:

\section{Related Work}
Creating an ranking results in a search engine can be done through many different algorithms. This related work section will explore the literature covering two types of algorithms used for ranking the documents based on the query: non-probabilistic and probabilistic. Then we will investigate how to evaluate the results, based on a test set. Deep Learning is also a new topic that has caught attention, and will therefore be reviewed in terms of how it can be used in search engines.

Web search today is a norm that the society, individuals, universities and business do on a daily basis. These systems are normally a traditional information retrieval (IR). The first web search engine was created in 1993, but soon the famous search engine Google was created by Larry Page and Sergey Brin. The trick that made Google better than the other search engines was of an algorithm called PageRank introduced by Page, L. in 1998 \cite{brin1998anatomy}. He explains the PageRank as "an objective measure of its citation importance that corresponds well with people’s subjective idea of importance." This ranking algorithm is still used by Google today.

Another method of ranking the importance of a website was proposed by Kleinberg, JM. in 1999 \cite{kleinberg1999authoritative}. The method he proposed is called HITS and is link analysis algorithm. He suggest the use of identifying hubs and authorities in a web graph where hubs serves as large directories without specific authoritative information, while the authorities will hold important information.

Both PageRank and HITS are algorithms that ranks the importance of a page, but does not take a query into consideration. Hence, these algorithms are good for ranking the difference between results you receive from the algorithms that take the query into consideration. Some algorithms commonly used are probabilistic and non-probabilistic. One of the earliest and simplest non-probabilistic algorithms was the Standard Boolean model introduced by Fayen, G. \cite{lancaster1973fayen}. Since this algorithm is based on exact match it will either give you too many or too few documents. Further on, more sophisticated algorithms came along such as the Vector Space Model \cite{salton1975vector}, TF-IDF \cite{salton1983mcgill}, query likelihood language model \cite{zhai2001model} etc.They all have a goal of retrieving the most relevant documents to a certain query.

IR research has always had a strong emphasis on measuring the efficacy of an IR system. This includes discovering the relevance of documents, retrieved by a search engine, relative to the user's information need. Evaluation is important in order to assess the performance of the system, measure the differences of multiple systems, and to learn the faults of a system with regards to improve the works.  The Text Retrieval Conference (TREC) is a conference emphasising on supporting and persuading the IR community through providing an infrastructure for evaluation IR systems. The famous infrastructure for evaluation, trec\_eval, is used for all ad hoc tasks in TREC \cite{voorhees:evaluation}. Two of the measurements are precision and recall, first developed by Spärck-Jones, K.\cite{jones1981information}. This type of evaluation is based on a document collection that includes documents which are classified as relevant or non-relevant, based on a set of queries.

On the other hand, it has shown that defining relevance is very hard when it comes to "real" users\cite{mizzaro1997relevance}. Therefore, another method of capturing user interactions with a search engine is Transaction Log Analysis (TLA). It is based on creating transaction logs to recognise attributes within the search process. It's a way of measuring the searcher's actions, the interaction between the user and the system along with the results \cite{glaser1967discovery}. 

Having a stable and robust test collection to help evaluate your IR model has been an important part of evaluation. It has created a more connected research society where researchers have shared test collections between each other. Mark Sanderson emphasises the powerfulness of good test collections used in conjunction with evaluation measures \cite{sanderson:evaluation}. One evaluation method mentioned by Sanderson is Discounted Cumulative Gain (DCG) and normalised DCG (nDCG)\cite{sanderson:evaluation}. DCG is often used to measure the ranking quality and the effectiveness of web-based search engines specifically. The algorithm measures the ‘gain’ of a document based on its position in the list. nDCG normalises the gain across queries, which is done by sorting the relevant documents by their relative relevance.

The very popular subject Deep Learning has also been used in web search engines. Huang, PS. \emph{et al.} proposes a deep structured semantic model that is trained by trying to reach the maximum conditional likelihood of a clicked document given a query \cite{huang2013learning}. The idea is based on click-through data collected which is used to feed the model in order to learn which pages are relevant to a query. Another paper by Deng, L. \emph{et al.} used deep stacking networks (DNS) in order to perform a parallel and scalable relevance prediction on an IR task \cite{deng2013deep}. It outperforms previous well known machine learning ranking algorithms such as LambdaRank and LambdaMART \cite{burges2010ranknet} in normalised DCG. 

Many of these algorithm are commonly used in web search today and the rest of the report will explore how some of these techniques can be used in a search engine. A comparison between different methods will also be provided.


% section related_work (end)


\section{Experiment} % (fold)
\label{sec:experiment}

\subsection{Dataset Description} % (fold)
\label{sub:dataset_description}

The search engine is dealing with two core datasets: the crawled web pages and the test set for evaluation. The crawled web pages are specifically modified and transformed for use. One example of this is creating an inverted index based on the content of the crawled web pages. Another example is how to create an adjacency matrix based on the relationship between pages.

The test set was manually created in order for us to evaluate the performance of our search engine. Two main test sets were created: one for precision, and one for recall. That means that one test set is specifically created such that the user want \emph{one} specific document (precision), while the other test set is created to retrieve as many relevant documents as possible (recall). Both test sets needed queries and relevant documents related to the queries. There are also three different types of queries: informational, transactional and navigational queries. The two test sets therefore includes all three types of queries.

% subsection dataset_description (end)

\subsection{Methods} % (fold)
\label{sub:methods}
The methodology includes a wide range of steps and exploration in order to build our search engine. These will be presented here.

\subsubsection{AWS Database \& Server Setup} % (fold)
\label{ssub:database_and_server_setup}

% subsubsection database_and_server_setup (end)

\subsubsection{Crawling} % (fold)
\label{ssub:crawling}

The crawling was done through an open source platform called Scrapy\footnote{https://scrapy.org} written in Python. This platform would be given a start-url "\url{www.cs.ucl.ac.uk}", followed by a depth-first-search on all links containing the "\url{cs.ucl.ac.uk}" domain. So "\url{www.cs.ucl.ac.uk/home}" would be accepted, just like "\url{www.wiki.cs.ucl.ac.uk}". We set a max-depth of the crawler because we realised throughout the crawling that some links were extremely long with unnecessary indexing information of the CS website.

The elements that were crawled from the website was the urls, titles ($<$h1$>$ tags) of page and the anchor text. Each instance would also be associated with a unique identifier. Let's call this table \emph{cs\_sites}. The reason why we didn't crawl the content of the pages to store in the database was because of storage reasons. As long as we have all the links of the whole domain in the database it is easy to access the content of the page at a later point.

% subsubsection crawling (end)

\subsubsection{Inverted Index} % (fold)
\label{ssub:inverted_index}

% subsubsection inverted_index (end)

\subsubsection{PageRank} % (fold)
\label{ssub:pagerank}

The popular algorithm PageRank\footnote{https://en.wikipedia.org/wiki/PageRank} is an algorithm developed by Google for ranking the results within a search engine. It is also commonly used for measuring the importance of a website. The algorithm is based on an assumption that more important websites are more likely to be pointed to from other websites.

The algorithm implies that it would need a way of representing connections between web sites. This can easily be done with the aid of an adjacency matrix. Firstly, all the connections needed to be measured, which was done by obtaining all the outgoing links for each site in \emph{cs\_sites}. Now, each link would point to a certain number of outgoing links disregarding whether they were in \emph{cs.ucl.ac.uk} domain or not. From these connection a adjacency matrix as shown as an example in Table \ref{fig:adj_mx}. 

\begin{table}[!h]
  \centering
  \begin{tabular}{|lr|c|c|c|} \cline{3-5}
  \multicolumn{1}{l}{} && \multicolumn{3}{c|}{To} \\ \cline{3-5}
  \multicolumn{1}{l}{} & & A & B & C  \\ \hline
  \multirow{3}{*}{\begin{sideways}From\end{sideways}}
  %                           A   B   C   
  & \multicolumn{1}{|r|}{A} & 0 & 0 & 1  \\ \cline{2-5}
  & \multicolumn{1}{|r|}{B} & 0 & 1 & 1  \\ \cline{2-5}
  & \multicolumn{1}{|r|}{C} & 1 & 1 & 0  \\ \hline
  \end{tabular}
  \caption{Adjacency matrix between links, where A, B and C represents links. The number 1 represents a connection, while 0 represents no connection.}
  \label{fig:adj_mx}
\end{table}

The adjacency matrix was then used in the PageRank algorithm to compute the rank of each link. The implementation would first convert the adjacency matrix into a markov matrix, followed by computing the rank of state \emph{i}:

$$ r_i = r_{i-1} \cdot (I_i *s + S_i * s + T_i * (1-s)) $$ 

where \emph{r} is the PageRank, \emph{$I_i$} is the inlinks of state \emph{i}, \emph{$S_i$} are the sink states, \emph{$T_i$} is the teleporting state and \emph{s} is the probabillity of following a transitions. That means that \emph{s-1} is the probability of teleporting. We have set $s=.85$. The algorithm will continue to calculate \emph{$r_i$} until it reaches a value below the maxerr rate ($maxerr = 0.001$). It is then said to be converged.


% subsubsection pagerank (end)

\subsubsection{Boolean Retrieval} % (fold)
\label{ssub:boolean_retrieval}

% subsubsection boolean_retrieval (end)

\subsubsection{TF-IDF} % (fold)
\label{sub:tf_idf}

TF-IDF is a type of ranking algorithm that is based off of reflecting how important a certain word is within a document in a collection. Normally, it is used as a weighting factor but can also be used for pure ranking given a query term. It is based on two concepts: the term frequency (TF) and the inverse document frequency (IDF). The two terms have been implemented in the search engine using these equations:

$$TF = \frac{f_{t,d}}{\sum\limits_{t' \in d} f_{t,d}}$$

$$IDF = log_2 \frac{N}{n_t}$$

where $f_{t,d}$ is the number of term occurrences within a document, while the summation is the length of the document. \emph{N} is the number of documents in collection and $n_t$ is the term frequency in the collection. The documents would then be ranked based on the product of $TF * IDF$ and will display all the documents in order. 

% subsection tf_idf (end)

\subsubsection{BM25} % (fold)
\label{ssub:BM25}

BM25 is another type of ranking algorithm developed by Karen Spärck Jones. It is a newer and improved version of TF-IDF. The BM25 belongs to the probabilistic models and uses the "bag of words" concept for retrieving relevant documents from a given search query. 

The difference is that it pays additional attention to the term frequency and the document length. Classic IDF has the potential for giving negative scores for terms with very high document frequency. On the other hand, the IDF in BM25 adds 1 to the value before taking the logarithm, which makes it impossible to compute negative values. Term frequency in BM25 dampens the impact of term frequency even further than traditional TF-IDF. The impact of term frequency is always increasing, but asymptotically approaches a value. In BM25, the TF score also depends on the document length. If the document length is below the average, then the TF score will get a bonus. 

The formula to adjust the TF score based on the document length is as below where b and k are constants and L is the length of document:
\[\frac{(k + 1) * TF}{k * (1.0 - b + b * L) + TF}\]

Adding up all the changes upon the classic TF-IDF, the formula of BM25 is as follow:
\[score(q,d)=IDF * \frac{(k + 1) * TF}{k * (1.0 - b + b * \frac{|d|}{avgDl}) + TF}
\]

% subsubsection BM25 (end)

\subsubsection{Query Likelihood with Smoothing} % (fold)
\label{ssub:query_likelihood_with_smoothing}

% subsubsection query_likelihood_with_smoothing (end)


% subsection methods (end)

\subsection{Metrics \& Analysis} % (fold)
\label{sub:metrics_&_analysis}

% subsection metrics_&_analysis (end)

\subsection{Analysis of Results} % (fold)
\label{sub:analysis_of_results}

% subsection analysis_of_results (end)

% section experiment (end)

\section{Discussion \& Limitations} % (fold)
\label{sec:discussion_&_limitations}

% section discussion_&_limitations (end)

\section{Conclusion} % (fold)
\label{sec:conclusion}

% section conclusion (end)

\section{References} % (fold)
\label{sec:references}

% section references (end)


% \begin{equation}
%   \lim_{n\rightarrow \infty}x=0
% \end{equation}
% Notice how it is formatted somewhat differently in
% the \textbf{displaymath}
% environment.  Now, we'll enter an unnumbered equation:
% \begin{displaymath}
%   \sum_{i=0}^{\infty} x + 1
% \end{displaymath}
% and follow it with another numbered equation:
% \begin{equation}
%   \sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f
% \end{equation}
% just to demonstrate \LaTeX's able handling of numbering.


% \begin{table}
%   \caption{Frequency of Special Characters}
%   \label{tab:freq}
%   \begin{tabular}{ccl}
%     \toprule
%     Non-English or Math&Frequency&Comments\\
%     \midrule
%     \O & 1 in 1,000& For Swedish names\\
%     $\pi$ & 1 in 5& Common in math\\
%     \$ & 4 in 5 & Used in business\\
%     $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
%   \bottomrule
% \end{tabular}
% \end{table}


% \begin{table*}
%   \caption{Some Typical Commands}
%   \label{tab:commands}
%   \begin{tabular}{ccl}
%     \toprule
%     Command &A Number & Comments\\
%     \midrule
%     \texttt{{\char'134}author} & 100& Author \\
%     \texttt{{\char'134}table}& 300 & For tables\\
%     \texttt{{\char'134}table*}& 400& For wider tables\\
%     \bottomrule
%   \end{tabular}
% \end{table*}
% % end the environment with {table*}, NOTE not {table}!

% Lists:
% \begin{enumerate}
% \item Never, ever use vertical rules.
% \item Never use double rules.
% \end{enumerate}
% It is also a good idea not to overuse horizontal rules.

% Figures
%\begin{figure}
%\includegraphics{fly}
%\caption{A sample black and white graphic.}
%\end{figure}
%
%\begin{figure}
%\includegraphics[height=1in, width=1in]{fly}
%\caption{A sample black and white graphic
%that has been resized with the \texttt{includegraphics} command.}
%\end{figure}


